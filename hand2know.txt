Preprocessing:

zorg ervoor dat er een gelijke hoeveelheid aan data is voor elke klasse

Sequential data moeten we opsplitsen in "sequences" van bijvoorbeeld telkens 50 rijen data (mbv from collections import deque)
window=deque(maxlen=50)
for row in rows
	window.append(row)
	if(len(window))==50):
		create new sequence
shuffle sequence

ftp://ftp.sas.com/pub/neural/FAQ2.html#A_std_target
Output data moet niet persé gestandardiseerd worden. Als er maar 1 output variabele is is dit zeker niet nodig. Als het meerdere outputs heeft, dan kan het wel nuttig zijn
in geval dat je als loss mean squared error neemt en 1 vd outputs veel groter is dan de andere

Fitting:

Met keras (analyse):
Als ge u model wilt optimalizeren voor u data, kunt ge tensorboard gebruiken om de resultaten te visualiseren.
https://www.youtube.com/watch?v=lV09_8432VA
Met tensorboard kun je ook modelcheckpoints gebruiken. Wanneer bijvoorbeeld de acc een nieuw record breekt, slaag die epoch op ipv alle epochs of om de x


LSTM (Long short term memory) voor sequential data (bij cpu versie duurt het leren precies wat lang)
GRU (Gated recurring? unit) voor sequential data
Passive Agressive zou goed moeten zijn voor online learning => warm_start op true zetten gebruikt vorige "weights" worden hergebruikt

t_cy das de phase da verschuifs want daar is de kracht het hoogst
t_dc_max reden: is bij elke motor zo beperkt vermogen; te hoog koppel => maakt kappot; te hoge snelheid => vliegt uit elkaar (ofwal weinig vermogen en hoog toerental; ofwel veel vermogen en weinig toeren; hetzelfde als in de werkelijkheid)
